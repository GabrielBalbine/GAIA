<p align="center">
  <img src="assets/gaia.gif" alt="Logo do Projeto GAIA" width="500"/>
</p>

<h1 align="center">Projeto GAIA</h1>

<p align="center">
  <em>Uma Ferramenta de Vis√£o Computacional para An√°lise Quantitativa de Padr√µes Comportamentais no Neurodesenvolvimento</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow" alt="Status do Projeto">
  <img src="https://img.shields.io/badge/Python-3.9-blue.svg" alt="Vers√£o do Python">
  <img src="https://img.shields.io/badge/License-MIT-green" alt="Licen√ßa">
</p>

---

## üìñ Sobre o Projeto

O diagn√≥stico de condi√ß√µes do neurodesenvolvimento, como o Transtorno do Espectro Autista (TEA), √© um processo complexo e predominantemente cl√≠nico, que depende de observa√ß√µes comportamentais. O **GAIA** (acr√¥nimo para *Gaze and Action Interaction Analyzer*) nasce como um projeto de TCC com o objetivo de criar uma ferramenta de suporte que traga mais **objetividade e dados quantitativos** para essa an√°lise.

Utilizando vis√£o computacional, o GAIA processa v√≠deos de intera√ß√µes (como entre pais e filhos) para extrair m√©tricas autom√°ticas de comportamento. A an√°lise √© focada em duas frentes principais:

1.  **Aten√ß√£o Visual:** Onde a crian√ßa est√° olhando? Quanto tempo ela passa fixada em rostos em compara√ß√£o com objetos? Para isso, o projeto implementar√° a detec√ß√£o de **√Åreas de Interesse (AOIs)**, mapeando o foco visual ao longo do tempo.
2.  **Movimento Corporal:** Como a crian√ßa se move e gesticula? O projeto utiliza **captura de pose (*pose estimation*)** para extrair o esqueleto corporal e permitir a futura an√°lise de padr√µes motores.

O objetivo final √© criar uma plataforma que possa ajudar pesquisadores e profissionais da sa√∫de a identificar padr√µes comportamentais sutis em diferentes grupos (como crian√ßas com TEA, neurot√≠picas e com tra√ßos de insensibilidade emocional), contribuindo para um entendimento mais profundo e auxiliando em interven√ß√µes futuras.

### ‚ú® Principais Funcionalidades (Atuais e Planejadas)

* ‚úÖ **Captura de Pose (*Pose Estimation*):** Detec√ß√£o dos 33 pontos-chave do corpo e gera√ß√£o de um v√≠deo com o esqueleto sobreposto. *(Etapa atual conclu√≠da)*
* ‚è≥ **An√°lise de Aten√ß√£o Visual:** Detec√ß√£o de rostos e defini√ß√£o de √Åreas de Interesse (AOIs) para rastrear o olhar. *(Pr√≥xima etapa)*
* ‚è≥ **Extra√ß√£o de M√©tricas:** Calcular dados como tempo de fixa√ß√£o em AOIs, frequ√™ncia de gestos, etc.
* ‚è≥ **Visualiza√ß√£o de Dados:** Gerar relat√≥rios e gr√°ficos com as m√©tricas extra√≠das.

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3.9**
* **OpenCV:** Para manipula√ß√£o de v√≠deo.
* **MediaPipe:** Para o modelo de IA de detec√ß√£o de pose e, futuramente, de faces e landmarks faciais.
* **Docker:** Para a cria√ß√£o de um ambiente de desenvolvimento 100% consistente e reprodut√≠vel.

---

## üöÄ Come√ßando

Para executar este projeto, voc√™ precisar√° ter o [Docker](https://www.docker.com/products/docker-desktop/) instalado.

### Instala√ß√£o

1.  **Clone o reposit√≥rio:** `git clone https://github.com/seu-usuario/seu-repositorio.git`
2.  **Navegue at√© a pasta:** `cd seu-repositorio`
3.  **Construa e inicie o cont√™iner Docker:**
    ```sh
    docker-compose up -d --build
    ```

---

##  Como Usar (M√≥dulo de Captura de Pose)

1.  **Prepare o v√≠deo:** Coloque um arquivo de v√≠deo (ex: `video_teste.mp4`) na raiz do projeto.
2.  **Ajuste o script:** Abra `captura_esqueleto.py` e altere a vari√°vel `input_video_path` para o nome do seu v√≠deo.
3.  **Execute a an√°lise:**
    ```sh
    docker-compose exec esqueleto-tracker python captura_esqueleto.py
    ```
4.  **Baixe o resultado:** O v√≠deo processado (`esqueleto_output.mp4`) aparecer√° na pasta, pronto para ser baixado.

---

## ‚úçÔ∏è Autores

* **Gabriel Balbine de Andrades**
* **Luiggi Paschoalini Garcia**

### Orientador

* **Prof. Dr. Victor Varela**

---

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT.
